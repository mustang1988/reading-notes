# 第一章

## 感知器

> [!info] 感知器
> 一种**接收多个**二进制数据进行计算后**输出一个**二进制数据的计算单元

## Sigmoid神经元(S神经元)

> [!info] Sigmoid神经元(S神经元)
> 一种**接收多个**(0,1)的数据进行计算后**输出一个**(0,1)数据的计算单元，计算方法为sigmoid函数
> > [!abstract] Sigmoid函数的数学表达
> > $$ \sigma(z) \equiv \frac{1}{1+e^{-z}} $$

- 神经网络
    - 由输入层，输出层和多个隐藏层，组成的网络，每层由多个神经元组成
- 梯度下降
    - 代价函数（二次代价函数）
    - ==梯度下降原理==
    - 二次代价函数+梯度下降的代码实现 MNIST
- 第二章
    - ==反向传播算法原理==
        - ==反向传播4个基本方程及其证明==
    - 反向传播算法的代码实现

- 第三章
    - 训练中发现的问题及其解决方案
        - 反常识的学习速度问题
            - 解决方案：更换代价函数，使用交叉熵代价函数替换二次代价函数
                - 交叉熵代价函数原理
                    - 解释，==交叉熵代价函数为何比二次代价函数更优的原因==
        - 过度拟合问题
            - 解决方案：权重衰减（L2规范化），在代价函数中加入额外项
                - 解释，规范化解决过度拟合问题的原理
                - 其他规范化方法
                    - L1规范化：在代价函数上加上一个权重绝对值的和
                    - 弃权：临时性地随机删除网络中的一半的隐藏神经元，再辅以一些平均或者投票的方式来选择输出结果
                    - 人为扩展训练数据：对已有训练数据做小幅变动后加入训练（MNIST中为按照固定角度旋转用于训练的手写字，以扩充训练集）
        - 权重初始化值对训练结果的影响
            - 优化权重初始化值：可以提升训练速度和性能
        - 使用代码实现权重初始化的优化与规范化
        - 训练用超参数的选择策略
            - 常用超参数包括
                - 训练速率
                - 规范化参数
                - 随机梯度下降的采样数
        - 其他优化训练的方法
            - Hessian技术
            - 基于momentum的梯度下降
            - 使用其他类型的神经元，替换Sigmoid神经元
                - tanh 神经元：Sigmoid神经元的按比例变化版本，其输出值范围从 0~1 变为 -1~1
                - ReLU: 修正线性神经元（rectifiedlinearneuron）或者修正线性单元（rectified linearunit）
- 第四章
    - 神经网络普遍性定理的理解
        - 包含**一个**隐藏层的神经网络可以被用来按照**任意**给定的精度来近似任何**连续函数**
    - 神经网络普遍性原理的局限性：通过数学证明可以确定，针对某种需求或问题**一定存在**一个可以"完美"解决该问题的神经网络，但是我们并没有一个行之有效的方法或技术，将这个神经网络给复现或构造出来
- 第五章
    - 在普遍性原理已经得到证明的当下，为何还需要引入深度神经网络？
        - 因为现实世界的问题复杂度太高，将复杂问题拆分为简单问题再对简单问题进行求解，是非常有效的方法论，因此在使用神经网络来解决现实世界问题时，也会对问题进行拆分，对每个拆分后的简单问题使用独立的小型的神经网络进行求解，再将这些小型神经网络进行串联与整合，便形成了深度神经网络，者可以大大降低神经网络的构建难度，而且简单的小型神经网络构建完成后，还可以实现复用，类似于编程中的函数
    - 为什么深度神经网络的训练很困难
        - 深度神经网络中使用基于梯度下降的学习方法本身存在着内在不稳定性：会导致深度神经网络出现前后层神经网络的学习速率不同，表现为：先前层学习效率较好时后面的层学习速率停滞不前或相反（即学习梯度消失）
        - 梯度消失与梯度激增问题产生的原因
            - 本质上梯度的消失或激增并不是问题，而只是问题的表象，问题的根源在于：
                - 前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景，唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡
                - 在使用sigmoid神经元时，梯度消失的现象会普遍存在
        - 深度学习中会遇到的其他问题
            - sigmoid函数会导致最终层上的激活函数在训练中会聚集在0，这也导致 了学习的缓慢。他们的工作中提出了一些取代sigmoid 函数的激活函数选择，使得不会被这种聚集性影响性能
            - 深度学习使用随机权重初始化和基于momentum的SGD 方法。两种情形下，好的选择可以获得较大的差异的训练效果
        - 总结，以下技术点都可以左右深度神经网络的学习速率和效果
            - 激活函数的选择
            - 权重的初始化方式
            - 学习算法的实现
            - 网络结构
            - 超参数的选择
              
             > [!warning] 目前还没有一个行之有效的方法论来指导上述技术点如何选择可以规避深度学习中产生的问题，学界仍在研究
  
  - 第六章
      - 可以用来训练深度神经网络的技术
          - 卷积
          - pooling
          - 使用GPU
          - 训练数据的算法性扩展
          - dropout技术
          - 网络的ensemble
          - 其他神经网络模型