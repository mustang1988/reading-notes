# 第一章

## 感知机

````ad-info
title: 感知机

一种**接收多个**二进制数据进行计算后**输出一个**二进制数据的计算单元

```ad-note
title: 感知机激活函数

$$ 
output = 
\left\{ 
\begin{array}{**lr**}  
1, & if & w·x+b\le{0} &  \\  
0, & if & w·x+b\gt{0} &  \\   
\end{array}  
\right.
$$

---

~~~functionplot
---
title: 
xLabel: z
yLabel: f(z)
bounds: [-5, 5, 0, 1.5]
disbaleZoom: false
grid: true
---
f(x)= x>0 ? 1 : 0
~~~

```
````

## Sigmoid神经元(S神经元)

````ad-info
title: S型(Sigmoid)神经元

一种**接收多个**(0,1)的数据进行计算后**输出一个**(0,1)数据的计算单元，计算方法为sigmoid函数

```ad-note
title: Sigmoid激活函数

$$ \sigma(z) \equiv \frac{1}{1+e^{-z}} $$

---

~~~functionplot
---
title: 
xLabel: Sigmoid(z)
yLabel: z
bounds: [-5, 5, 0, 1.5]
disbaleZoom: false
grid: true
---
f(x)=1/(1+E^(-x))
~~~
```
````

```ad-tip
title: 感知机扩展

因为感知机具备计算基本的逻辑功能, 可以使用感知机来模拟一个二极管电路,来实现与门,或门,非门这些"逻辑电路"
```

## 神经网络架构

![[assets/images/Pasted image 20230207153630.png]]

一个神经网络由以下三部分组成
- 输入层
    - 位于神经网络图示的最左边，其中的神经元被称为**输入神经元**
- 隐藏层
    - 既非输入也非输出的层均为隐藏层，在一个神经网络中，隐藏层的数量是不定的，***至少会有一个***隐藏层，当隐藏层的数量**多余**一个时，可以成这个神经网络为 **[[notes/神经网络和深度学习#^7ehmosngoig|深度神经网络]]**， 有时被称为多层感知器或者MLP
- 输出层
    - 位于神经网络图示的最右边， 其中的神经元被称为**输出神经元**

## 梯度下降

### 代价函数

#### 均方误差代价函数

> [!abstract]+ 均方误差代价函数的数学表达
> $$ C(w,b) \equiv 1/2n \sum_{x} \left|\left| y(x)-a \right|\right|^{2} $$

### 梯度下降及其原理

> [!warning]+ ==!!待加强理解==
> 涉及知识点：
> - 向量的点积
> - 偏导数
> - 梯度
> - 向量的转置


## 均方误差代价函数+梯度下降的代码实现 MNIST

略

---
# 第二章

## 反向传播算法

> [!warning]+ ==!!待加强理解==
> 涉及知识点
> - 反向传播的四个基本方程及其推导及证明过程

## 反向传播算法的代码实现

略

---

# 第三章

## 训练中发现的问题及其解决方案

### 反常识的学习效率问题

> [!abstract]+ 问题表象
> 人工神经元在其犯错较大的训练初期，其学习效率较低，在训练末期，正确率较高时，学习效率反而较高

> [!important]+ 问题解决方案
> 将训练用代价函数，从均方误差代价函数替换为交叉熵代价函数

#### 交叉熵代价函数

TODO

##### 为何交叉熵代价函数比均方误差代价函数在发生反常识学习效率问题时更优秀

TODO

### 过度拟合问题

> [!abstract]+ 问题表象
> 训练后的神经网络在训练数据上测试得到的准确率较高，但使用非训练数据集对其测试，准确率低很多

> [!important]+ 问题解决方案
> 使用规范化

#### 规范化

TODO 规范化的含义

##### 规范化方法

- L1规范化
    - 在代价函数上加上一个权重绝对值的和
        - TODO
- L2规范化（权重衰减）
    - 增加一个额外的项到代价函数上，这个项叫做**规范化项**
- 弃权
    - 临时性地随机删除网络中的一半的隐藏神经元，再辅以一些平均或者投票的方式来选择输出结果
- 人为增加训练样本
    - 对已有训练数据做小幅变动后加入训练（MNIST中为按照固定角度旋转用于训练的手写字，以扩充训练集）

#### 权重初始化值对训练的影响

优化权重初始化值：可以提升训练速度和性能


#### 规范化，优化权重初始化值的代码实现

TODO

#### 训练超参数值的选择

- 常用超参数包括
    - 训练速率
    - 规范化参数
    - 随机梯度下降的采样数

#### 其他优化神经网络的技术

- 随机梯度下降的变化形式
    - Hessian技术
    - 基于momentum的梯度下降
- 使用其他激活函数的神经元
    - tanh神经元
        - Sigmoid神经元的按比例变化版本，其输出值范围从 0~1 变为 -1~1
    - ReLU神经元，修正线性神经元（rectifiedlinearneuron）或者修正线性单元（rectified linearunit）
        - TODO

---

# 第四章

## 神经网络普遍性定理的理解
    
包含**一个**隐藏层的神经网络可以被用来按照**任意**给定的精度来近似任何**连续函数**

神经网络普遍性原理的局限性：通过数学证明可以确定，针对某种需求或问题**一定存在**一个可以"完美"解决该问题的神经网络，但是我们并没有一个行之有效的方法或技术，将这个神经网络给复现或构造出来

---
# 第五章
## 在普遍性原理已经得到证明的当下，为何还需要引入深度神经网络？

因为现实世界的问题复杂度太高，将复杂问题拆分为简单问题再对简单问题进行求解，是非常有效的方法论，因此在使用神经网络来解决现实世界问题时，也会对问题进行拆分，对每个拆分后的简单问题使用独立的小型的神经网络进行求解，再将这些小型神经网络进行串联与整合，便形成了深度神经网络，者可以大大降低神经网络的构建难度，而且简单的小型神经网络构建完成后，还可以实现复用，类似于编程中的函数

## 为什么深度神经网络的训练很困难


        - 深度神经网络中使用基于梯度下降的学习方法本身存在着内在不稳定性：会导致深度神经网络出现前后层神经网络的学习速率不同，表现为：先前层学习效率较好时后面的层学习速率停滞不前或相反（即学习梯度消失）
        - 梯度消失与梯度激增问题产生的原因
            - 本质上梯度的消失或激增并不是问题，而只是问题的表象，问题的根源在于：
                - 前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景，唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡
                - 在使用sigmoid神经元时，梯度消失的现象会普遍存在
        - 深度学习中会遇到的其他问题
            - sigmoid函数会导致最终层上的激活函数在训练中会聚集在0，这也导致 了学习的缓慢。他们的工作中提出了一些取代sigmoid 函数的激活函数选择，使得不会被这种聚集性影响性能
            - 深度学习使用随机权重初始化和基于momentum的SGD 方法。两种情形下，好的选择可以获得较大的差异的训练效果
        - 总结，以下技术点都可以左右深度神经网络的学习速率和效果
            - 激活函数的选择
            - 权重的初始化方式
            - 学习算法的实现
            - 网络结构
            - 超参数的选择
              
             > [!warning] 目前还没有一个行之有效的方法论来指导上述技术点如何选择可以规避深度学习中产生的问题，学界仍在研究
---
- 第六章
  - 可以用来训练深度神经网络的技术
      - 卷积
      - pooling
      - 使用GPU
      - 训练数据的算法性扩展
      - dropout技术
      - 网络的ensemble
      - 其他神经网络模型