---
annotation-target: "[[assets/neural networks and deep learning.pdf]]"
---

>%%
>```annotation-json
>{"created":"2023-01-30T09:58:58.413Z","text":"TODO\n\n- Q1:\n\n    为何此处所给的公式与MSE公式的分母不同，MSE公式分母为n，而此处为 2n ？\n\n    https://zhuanlan.zhihu.com/p/435515042","updated":"2023-01-30T09:58:58.413Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":19865,"end":19890},{"type":"TextQuoteSelector","exact":"C(w, b) ≡ 12n∑x∥y(x) −a∥2","prefix":"练输入x。为了量化我们如何实现这个目标，我们定义一个代价函数3：","suffix":" (6)这里w 表示所有的网络中权重的集合，b 是所有的偏置，n"}]}]}
>```
>%%
>*%%PREFIX%%练输入x。为了量化我们如何实现这个目标，我们定义一个代价函数3：%%HIGHLIGHT%% ==C(w, b) ≡ 12n∑x∥y(x) −a∥2== %%POSTFIX%%(6)这里w 表示所有的网络中权重的集合，b 是所有的偏置，n*
>%%LINK%%[[#^i1evx8vph4c|show annotation]]
>%%COMMENT%%
>TODO
>
>- Q1:
>
>    为何此处所给的公式与MSE公式的分母不同，MSE公式分母为n，而此处为 2n ？
>
>    https://zhuanlan.zhihu.com/p/435515042
>%%TAGS%%
>#公式解释, #存疑
^i1evx8vph4c



>%%
>```annotation-json
>{"created":"2023-01-30T10:08:24.765Z","text":"TODO","updated":"2023-01-30T10:08:24.765Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":10518,"end":10520},{"type":"TextQuoteSelector","exact":"点乘","prefix":"们可以创建两个符号的变动来简化。第一个变动是把∑jwjxj改写成","suffix":"，w ·x ≡∑jwjxj，这里w 和x 对应权重和输31.1."}]}]}
>```
>%%
>*%%PREFIX%%们可以创建两个符号的变动来简化。第一个变动是把∑jwjxj改写成%%HIGHLIGHT%% ==点乘== %%POSTFIX%%，w ·x ≡∑jwjxj，这里w 和x 对应权重和输31.1.*
>%%LINK%%[[#^dhm4em7g0tj|show annotation]]
>%%COMMENT%%
>TODO
>%%TAGS%%
>#名词解释
^dhm4em7g0tj


>%%
>```annotation-json
>{"created":"2023-01-31T01:41:54.943Z","text":"Mean Squared Error\n\n均方誤差","updated":"2023-01-31T01:41:54.943Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":20056,"end":20059},{"type":"TextQuoteSelector","exact":"MSE","prefix":"v 的模。我们把C 称为二次代价函数；有时也称被称为均方误差或者","suffix":"。观察二次代价函数的形式我们可以看到C(w, b) 是非负的，因"}]}]}
>```
>%%
>*%%PREFIX%%v 的模。我们把C 称为二次代价函数；有时也称被称为均方误差或者%%HIGHLIGHT%% ==MSE== %%POSTFIX%%。观察二次代价函数的形式我们可以看到C(w, b) 是非负的，因*
>%%LINK%%[[#^ptq2cymy6uk|show annotation]]
>%%COMMENT%%
>Mean Squared Error
>
>均方誤差
>%%TAGS%%
>#名词解释
^ptq2cymy6uk


>%%
>```annotation-json
>{"created":"2023-01-31T01:50:54.326Z","text":"TODO","updated":"2023-01-31T01:50:54.326Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":24281,"end":24290},{"type":"TextQuoteSelector","exact":"柯西-施瓦茨不等式","prefix":"向上做微小变化的方法。练习• 证明上一段落的推断。提示：可以利用","suffix":"。• 我已经解释了当C 是二元及其多元函数的情况。那如果C 是一"}]}]}
>```
>%%
>*%%PREFIX%%向上做微小变化的方法。练习• 证明上一段落的推断。提示：可以利用%%HIGHLIGHT%% ==柯西-施瓦茨不等式== %%POSTFIX%%。• 我已经解释了当C 是二元及其多元函数的情况。那如果C 是一*
>%%LINK%%[[#^p136th7n23|show annotation]]
>%%COMMENT%%
>TODO
>%%TAGS%%
>
^p136th7n23


>%%
>```annotation-json
>{"created":"2023-01-31T03:24:27.440Z","text":"反向传播假设1","updated":"2023-01-31T03:24:27.440Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":50136,"end":50185},{"type":"TextQuoteSelector","exact":"第一个假设就是代价函数可以被写成一个在每个训练样本x 上的代价函数Cx的均值C = 1n∑xCx。","prefix":"为了应用反向传播，我们需要对代价函数C 做出什么样的前提假设呢？","suffix":"这是关于二次代价函数的例子，其中对每个独立的训练样本其代价是Cx"}]}]}
>```
>%%
>*%%PREFIX%%为了应用反向传播，我们需要对代价函数C 做出什么样的前提假设呢？%%HIGHLIGHT%% ==第一个假设就是代价函数可以被写成一个在每个训练样本x 上的代价函数Cx的均值C = 1n∑xCx。== %%POSTFIX%%这是关于二次代价函数的例子，其中对每个独立的训练样本其代价是Cx*
>%%LINK%%[[#^dsze2i83dh7|show annotation]]
>%%COMMENT%%
>反向传播假设1
>%%TAGS%%
>#反向传播
^dsze2i83dh7


>%%
>```annotation-json
>{"created":"2023-01-31T03:24:51.998Z","text":"反向传播假设2","updated":"2023-01-31T03:24:51.998Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":50417,"end":50439},{"type":"TextQuoteSelector","exact":"第二个假设就是代价可以写成神经网络输出的函数","prefix":"看做C。最终我们会把下标加上，现在为了简化表示其实没有这个必要。","suffix":"：1其实，这就是让我们使用之前的矩阵下标wljk表示的初因。如果"}]}]}
>```
>%%
>*%%PREFIX%%看做C。最终我们会把下标加上，现在为了简化表示其实没有这个必要。%%HIGHLIGHT%% ==第二个假设就是代价可以写成神经网络输出的函数== %%POSTFIX%%：1其实，这就是让我们使用之前的矩阵下标wljk表示的初因。如果*
>%%LINK%%[[#^ib6eo9lth3|show annotation]]
>%%COMMENT%%
>反向传播假设2
>%%TAGS%%
>#反向传播
^ib6eo9lth3


>%%
>```annotation-json
>{"created":"2023-01-31T05:37:05.586Z","text":"神经网络的普遍性定义","updated":"2023-01-31T05:37:05.586Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":140989,"end":141024},{"type":"TextQuoteSelector","exact":"包含一个隐藏层的神经网络可以被用来按照任意给定的精度来近似任何连续函数","prefix":"常不是一个严重的限制。总结一下，更加准确的关于普遍性定理的表述是","suffix":"。本章，我们会使用了两个隐藏层的网络来证明这个结果的弱化版本。在"}]}]}
>```
>%%
>*%%PREFIX%%常不是一个严重的限制。总结一下，更加准确的关于普遍性定理的表述是%%HIGHLIGHT%% ==包含一个隐藏层的神经网络可以被用来按照任意给定的精度来近似任何连续函数== %%POSTFIX%%。本章，我们会使用了两个隐藏层的网络来证明这个结果的弱化版本。在*
>%%LINK%%[[#^6vfko1k7e6c|show annotation]]
>%%COMMENT%%
>神经网络的普遍性定义
>%%TAGS%%
>
^6vfko1k7e6c
