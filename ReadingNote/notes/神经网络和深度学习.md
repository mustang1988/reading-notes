---
annotation-target: "[[assets/neural networks and deep learning.pdf]]"
---

>%%
>```annotation-json
>{"created":"2023-01-30T09:58:58.413Z","text":"TODO\n\n- Q1:\n\n    为何此处所给的公式与MSE公式的分母不同，MSE公式分母为n，而此处为 2n ？\n\n    https://zhuanlan.zhihu.com/p/435515042","updated":"2023-01-30T09:58:58.413Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":19865,"end":19890},{"type":"TextQuoteSelector","exact":"C(w, b) ≡ 12n∑x∥y(x) −a∥2","prefix":"练输入x。为了量化我们如何实现这个目标，我们定义一个代价函数3：","suffix":" (6)这里w 表示所有的网络中权重的集合，b 是所有的偏置，n"}]}]}
>```
>%%
>*%%PREFIX%%练输入x。为了量化我们如何实现这个目标，我们定义一个代价函数3：%%HIGHLIGHT%% ==C(w, b) ≡ 12n∑x∥y(x) −a∥2== %%POSTFIX%%(6)这里w 表示所有的网络中权重的集合，b 是所有的偏置，n*
>%%LINK%%[[#^i1evx8vph4c|show annotation]]
>%%COMMENT%%
>TODO
>
>- Q1:
>
>    为何此处所给的公式与MSE公式的分母不同，MSE公式分母为n，而此处为 2n ？
>
>    https://zhuanlan.zhihu.com/p/435515042
>%%TAGS%%
>#公式解释, #存疑
^i1evx8vph4c



>%%
>```annotation-json
>{"created":"2023-01-30T10:08:24.765Z","text":"TODO","updated":"2023-01-30T10:08:24.765Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":10518,"end":10520},{"type":"TextQuoteSelector","exact":"点乘","prefix":"们可以创建两个符号的变动来简化。第一个变动是把∑jwjxj改写成","suffix":"，w ·x ≡∑jwjxj，这里w 和x 对应权重和输31.1."}]}]}
>```
>%%
>*%%PREFIX%%们可以创建两个符号的变动来简化。第一个变动是把∑jwjxj改写成%%HIGHLIGHT%% ==点乘== %%POSTFIX%%，w ·x ≡∑jwjxj，这里w 和x 对应权重和输31.1.*
>%%LINK%%[[#^dhm4em7g0tj|show annotation]]
>%%COMMENT%%
>TODO
>%%TAGS%%
>#名词解释
^dhm4em7g0tj


>%%
>```annotation-json
>{"created":"2023-01-31T01:41:54.943Z","text":"Mean Squared Error\n\n均方誤差","updated":"2023-01-31T01:41:54.943Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":20056,"end":20059},{"type":"TextQuoteSelector","exact":"MSE","prefix":"v 的模。我们把C 称为二次代价函数；有时也称被称为均方误差或者","suffix":"。观察二次代价函数的形式我们可以看到C(w, b) 是非负的，因"}]}]}
>```
>%%
>*%%PREFIX%%v 的模。我们把C 称为二次代价函数；有时也称被称为均方误差或者%%HIGHLIGHT%% ==MSE== %%POSTFIX%%。观察二次代价函数的形式我们可以看到C(w, b) 是非负的，因*
>%%LINK%%[[#^ptq2cymy6uk|show annotation]]
>%%COMMENT%%
>Mean Squared Error
>
>均方誤差
>%%TAGS%%
>#名词解释
^ptq2cymy6uk


>%%
>```annotation-json
>{"created":"2023-01-31T01:50:54.326Z","text":"TODO","updated":"2023-01-31T01:50:54.326Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":24281,"end":24290},{"type":"TextQuoteSelector","exact":"柯西-施瓦茨不等式","prefix":"向上做微小变化的方法。练习• 证明上一段落的推断。提示：可以利用","suffix":"。• 我已经解释了当C 是二元及其多元函数的情况。那如果C 是一"}]}]}
>```
>%%
>*%%PREFIX%%向上做微小变化的方法。练习• 证明上一段落的推断。提示：可以利用%%HIGHLIGHT%% ==柯西-施瓦茨不等式== %%POSTFIX%%。• 我已经解释了当C 是二元及其多元函数的情况。那如果C 是一*
>%%LINK%%[[#^p136th7n23|show annotation]]
>%%COMMENT%%
>TODO
>%%TAGS%%
>
^p136th7n23


>%%
>```annotation-json
>{"created":"2023-01-31T03:24:27.440Z","text":"反向传播假设1","updated":"2023-01-31T03:24:27.440Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":50136,"end":50185},{"type":"TextQuoteSelector","exact":"第一个假设就是代价函数可以被写成一个在每个训练样本x 上的代价函数Cx的均值C = 1n∑xCx。","prefix":"为了应用反向传播，我们需要对代价函数C 做出什么样的前提假设呢？","suffix":"这是关于二次代价函数的例子，其中对每个独立的训练样本其代价是Cx"}]}]}
>```
>%%
>*%%PREFIX%%为了应用反向传播，我们需要对代价函数C 做出什么样的前提假设呢？%%HIGHLIGHT%% ==第一个假设就是代价函数可以被写成一个在每个训练样本x 上的代价函数Cx的均值C = 1n∑xCx。== %%POSTFIX%%这是关于二次代价函数的例子，其中对每个独立的训练样本其代价是Cx*
>%%LINK%%[[#^dsze2i83dh7|show annotation]]
>%%COMMENT%%
>反向传播假设1
>%%TAGS%%
>#反向传播
^dsze2i83dh7


>%%
>```annotation-json
>{"created":"2023-01-31T03:24:51.998Z","text":"反向传播假设2","updated":"2023-01-31T03:24:51.998Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":50417,"end":50439},{"type":"TextQuoteSelector","exact":"第二个假设就是代价可以写成神经网络输出的函数","prefix":"看做C。最终我们会把下标加上，现在为了简化表示其实没有这个必要。","suffix":"：1其实，这就是让我们使用之前的矩阵下标wljk表示的初因。如果"}]}]}
>```
>%%
>*%%PREFIX%%看做C。最终我们会把下标加上，现在为了简化表示其实没有这个必要。%%HIGHLIGHT%% ==第二个假设就是代价可以写成神经网络输出的函数== %%POSTFIX%%：1其实，这就是让我们使用之前的矩阵下标wljk表示的初因。如果*
>%%LINK%%[[#^ib6eo9lth3|show annotation]]
>%%COMMENT%%
>反向传播假设2
>%%TAGS%%
>#反向传播
^ib6eo9lth3


>%%
>```annotation-json
>{"created":"2023-01-31T05:37:05.586Z","text":"神经网络的普遍性定义","updated":"2023-01-31T05:37:05.586Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":140989,"end":141024},{"type":"TextQuoteSelector","exact":"包含一个隐藏层的神经网络可以被用来按照任意给定的精度来近似任何连续函数","prefix":"常不是一个严重的限制。总结一下，更加准确的关于普遍性定理的表述是","suffix":"。本章，我们会使用了两个隐藏层的网络来证明这个结果的弱化版本。在"}]}]}
>```
>%%
>*%%PREFIX%%常不是一个严重的限制。总结一下，更加准确的关于普遍性定理的表述是%%HIGHLIGHT%% ==包含一个隐藏层的神经网络可以被用来按照任意给定的精度来近似任何连续函数== %%POSTFIX%%。本章，我们会使用了两个隐藏层的网络来证明这个结果的弱化版本。在*
>%%LINK%%[[#^6vfko1k7e6c|show annotation]]
>%%COMMENT%%
>神经网络的普遍性定义
>%%TAGS%%
>
^6vfko1k7e6c


>%%
>```annotation-json
>{"created":"2023-02-01T01:12:22.666Z","text":"神经网络普遍性**定理**\n\n该普遍性已得到证明","updated":"2023-02-01T01:12:22.666Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":139109,"end":139142},{"type":"TextQuoteSelector","exact":"不论我们想要计算什么样的函数，我们都确信存在一个神经网络可以计算它","prefix":",x3)f1(x1,x2,x3)结果表明神经网络拥有一种普遍性。","suffix":"。而且，这个普遍性定理甚至在我们限制了神经网络只在输入层和输出层"}]}]}
>```
>%%
>*%%PREFIX%%,x3)f1(x1,x2,x3)结果表明神经网络拥有一种普遍性。%%HIGHLIGHT%% ==不论我们想要计算什么样的函数，我们都确信存在一个神经网络可以计算它== %%POSTFIX%%。而且，这个普遍性定理甚至在我们限制了神经网络只在输入层和输出层*
>%%LINK%%[[#^ilgn2qyd67h|show annotation]]
>%%COMMENT%%
>神经网络普遍性**定理**
>
>该普遍性已得到证明
>%%TAGS%%
>
^ilgn2qyd67h


>%%
>```annotation-json
>{"created":"2023-02-01T01:36:07.613Z","text":"神经网络普遍性定理的两个预先声明1/2","updated":"2023-02-01T01:36:07.613Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":140495,"end":140510},{"type":"TextQuoteSelector","exact":"我们可以获得尽可能好的一个近似","prefix":"一点，这句话不是说一个网络可以被用来准确地计算任何函数。而是说，","suffix":"。通过增加隐藏元的数量，我们可以提升近似的精度。例如，前面我举例"}]}]}
>```
>%%
>*%%PREFIX%%一点，这句话不是说一个网络可以被用来准确地计算任何函数。而是说，%%HIGHLIGHT%% ==我们可以获得尽可能好的一个近似== %%POSTFIX%%。通过增加隐藏元的数量，我们可以提升近似的精度。例如，前面我举例*
>%%LINK%%[[#^ll4tboe8q6|show annotation]]
>%%COMMENT%%
>神经网络普遍性定理的两个预先声明1/2
>%%TAGS%%
>
^ll4tboe8q6


>%%
>```annotation-json
>{"created":"2023-02-01T01:37:52.389Z","text":"神经网络普遍性定理的两个预先声明2/2","updated":"2023-02-01T01:37:52.389Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":140817,"end":140861},{"type":"TextQuoteSelector","exact":"如果函数不是连续的，也就是会有突然、极陡的跳跃，那么一般来说无法使用一个神经网络进行近似","prefix":"内的。第二点，就是可以按照上面的方式近似的函数类其实是连续函数。","suffix":"。这并不意外，因为神经网络计算的就是输入的连续函数。然而，即使那"}]}]}
>```
>%%
>*%%PREFIX%%内的。第二点，就是可以按照上面的方式近似的函数类其实是连续函数。%%HIGHLIGHT%% ==如果函数不是连续的，也就是会有突然、极陡的跳跃，那么一般来说无法使用一个神经网络进行近似== %%POSTFIX%%。这并不意外，因为神经网络计算的就是输入的连续函数。然而，即使那*
>%%LINK%%[[#^p2cgzfsja18|show annotation]]
>%%COMMENT%%
>神经网络普遍性定理的两个预先声明2/2
>%%TAGS%%
>
^p2cgzfsja18


>%%
>```annotation-json
>{"created":"2023-02-01T01:51:26.587Z","text":"导致神经网络普遍性定理无法做到完美的问题点\n\n当调整权重w和偏执b来改变激活函数的表现趋近于越阶函数时存在无法完美拟合的\"拐点\"\n\n这个\"拐点\"所在的区间就是故障窗口，当输入给神经元的计算值处于故障窗口中时，神经元的计算结果会于实际值产生偏差，因此神经网络无法做到\"完美\"的普遍性","updated":"2023-02-01T01:51:26.587Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":151256,"end":151260},{"type":"TextQuoteSelector","exact":"故障窗口","prefix":"1354.5. 修补阶跃函数0 11x顶部隐藏神经元的输出在这些","suffix":"中我给出的普遍性的解释会失败。现在，它不是一个很严重的故障。通过"}]}]}
>```
>%%
>*%%PREFIX%%1354.5. 修补阶跃函数0 11x顶部隐藏神经元的输出在这些%%HIGHLIGHT%% ==故障窗口== %%POSTFIX%%中我给出的普遍性的解释会失败。现在，它不是一个很严重的故障。通过*
>%%LINK%%[[#^p61s9hohvhe|show annotation]]
>%%COMMENT%%
>导致神经网络普遍性定理无法做到完美的问题点
>
>当调整权重w和偏执b来改变激活函数的表现趋近于越阶函数时存在无法完美拟合的"拐点"
>
>这个"拐点"所在的区间就是故障窗口，当输入给神经元的计算值处于故障窗口中时，神经元的计算结果会于实际值产生偏差，因此神经网络无法做到"完美"的普遍性
>%%TAGS%%
>
^p61s9hohvhe


>%%
>```annotation-json
>{"created":"2023-02-01T01:57:59.480Z","text":"为何仅使用但隐藏层的神经网络就可以解决\"任意\"问题，还要引入多层的深度神经网络的主要原因","updated":"2023-02-01T01:57:59.480Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":152533,"end":152576},{"type":"TextQuoteSelector","exact":"深度网络有一个分级结构，使其尤其适用于学习分级的知识，这看上去可用于解决现实世界的问题","prefix":"这是可能的，使用深度网络仍然有实际的原因。正如在第一章中表明过，","suffix":"。但是更具体地，当攻克诸如图像识别的问题，使用一个不仅能理解单独"}]}]}
>```
>%%
>*%%PREFIX%%这是可能的，使用深度网络仍然有实际的原因。正如在第一章中表明过，%%HIGHLIGHT%% ==深度网络有一个分级结构，使其尤其适用于学习分级的知识，这看上去可用于解决现实世界的问题== %%POSTFIX%%。但是更具体地，当攻克诸如图像识别的问题，使用一个不仅能理解单独*
>%%LINK%%[[#^a3yqxpedit8|show annotation]]
>%%COMMENT%%
>为何仅使用但隐藏层的神经网络就可以解决"任意"问题，还要引入多层的深度神经网络的主要原因
>%%TAGS%%
>
^a3yqxpedit8


>%%
>```annotation-json
>{"created":"2023-02-01T02:55:26.124Z","text":"深度神经网络梯度不稳定问题的核心原因","updated":"2023-02-01T02:55:26.124Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":163144,"end":163227},{"type":"TextQuoteSelector","exact":"前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡","prefix":"题：根本的问题其实并非是消失的梯度问题或者激增的梯度问题，而是在","suffix":"。如果没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不"}]}]}
>```
>%%
>*%%PREFIX%%题：根本的问题其实并非是消失的梯度问题或者激增的梯度问题，而是在%%HIGHLIGHT%% ==前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡== %%POSTFIX%%。如果没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不*
>%%LINK%%[[#^po0fcvv59p|show annotation]]
>%%COMMENT%%
>深度神经网络梯度不稳定问题的核心原因
>%%TAGS%%
>
^po0fcvv59p


>%%
>```annotation-json
>{"created":"2023-02-01T03:48:14.430Z","text":"sigmoid 函数的数学表达","updated":"2023-02-01T03:48:14.430Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":13293,"end":13308},{"type":"TextQuoteSelector","exact":"σ(z) ≡ 11 + e−z","prefix":"现在是σ(w ·x + b)，这里σ 被称为S型函数1，定义为：","suffix":"(3)把它们放在一起来更清楚地说明，一个具有输入x1, x2, "}]}]}
>```
>%%
>*%%PREFIX%%现在是σ(w ·x + b)，这里σ 被称为S型函数1，定义为：%%HIGHLIGHT%% ==σ(z) ≡ 11 + e−z== %%POSTFIX%%(3)把它们放在一起来更清楚地说明，一个具有输入x1, x2,*
>%%LINK%%[[#^n8rmd56xd2d|show annotation]]
>%%COMMENT%%
>sigmoid 函数的数学表达
>%%TAGS%%
>
^n8rmd56xd2d


>%%
>```annotation-json
>{"created":"2023-02-01T08:48:20.961Z","text":"次函数也被称作神经元的 **激活函数**","updated":"2023-02-01T08:48:20.961Z","document":{"title":"neural networks and deep learning.pdf","link":[{"href":"urn:x-pdf:c27b805d8ea7529f8de84e4585f6d06b"},{"href":"vault:/assets/neural networks and deep learning.pdf"}],"documentFingerprint":"c27b805d8ea7529f8de84e4585f6d06b"},"uri":"vault:/assets/neural networks and deep learning.pdf","target":[{"source":"vault:/assets/neural networks and deep learning.pdf","selector":[{"type":"TextPositionSelector","start":13285,"end":13287},{"type":"TextQuoteSelector","exact":"函数","prefix":" 或1。相反，它现在是σ(w ·x + b)，这里σ 被称为S型","suffix":"1，定义为：σ(z) ≡ 11 + e−z(3)把它们放在一起来"}]}]}
>```
>%%
>*%%PREFIX%%或1。相反，它现在是σ(w ·x + b)，这里σ 被称为S型%%HIGHLIGHT%% ==函数== %%POSTFIX%%1，定义为：σ(z) ≡ 11 + e−z(3)把它们放在一起来*
>%%LINK%%[[#^bpjzd27lre4|show annotation]]
>%%COMMENT%%
>次函数也被称作神经元的 **激活函数**
>%%TAGS%%
>
^bpjzd27lre4
